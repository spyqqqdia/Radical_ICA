<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8"/>
	<meta name="andforcesustotitle" content="Linux Administration"/>
	<link type="text/css" rel="stylesheet" href="docs/css/bootstrap_min.css"/>
	<link type="text/css" rel="stylesheet" href="docs/css/tiny.css"/>
	<meta name="author" content="Michael J. Meyer"/>
	<meta name="date" content="June 24, 2016"/>
</head>
<body>

<h1 id="radicalica">Radical ICA</h1>

<p>This project implements the independent component analysis (<code>ICA</code>) algorithm using Scala 2.11.8 and JRE 1.8.0_76 with IntelliJ 2016.1.2.<br/>
Original authors of the algorithm: Eric G. Learned-Miller, John W. Fisher III.</p>

<ul>
<li><a href="docs/radical_ica.pdf">Introduction</a></li>
<li><a href="docs/RadicalICA_MilFish.pdf">Radical ICA, original paper</a></li>
<li><a href="docs/index.html">Scaladoc</a></li>
<li><a href="https://people.cs.umass.edu/~elm/">Prof. Learned-Miller, homepage</a></li>
<li><a href="https://people.cs.umass.edu/~elm/papers_by_code.html">Prof. Learned-Miller, R, Matlab code</a></li>
<li><a href="https://people.cs.umass.edu/~elm/ICA/">Radical ICA hompage at umass</a></li>
</ul>

<p>Independent component analysis is a search for a rotation of whitened multidimensional data (covariance matrix is the identity matrix)<br/>
which increases the independence of the components (coordinates). </p>

<p>The rotation is chosen so as to minimize a contrast function. The contrast function is the sum of the marginal entropies<br/>
(entropies of the components) of the sample plus a constant. Therefore an entropy estimator has to be employed.<br/>
The original algorithm uses the Vasicek entropy estimator based on the order statistic which makes it necessary to sort the components<br/>
of the sample. This is O(n*log(n)) in the sample size n.</p>

<p>The Radical ICA algorithm performs an exhaustive search and is therefore naturally slow. Here an attempt is made to speed this up<br/>
as much as possible. To this end a parallelized implementation is provided. In addition the empirical entropy estimator is provided<br/>
as an alternative to the Vasicek estimator. The algorithm is</p>

<ul>
<li>O(dim²nlog(n)) with the Vasicek entropy estimator and</li>
<li>O(dim²n) with the empirical entropy estimator,</li>
</ul>

<p>where dim is the dimension of the data. With a sample of size 30000 in dimension 6 the version using the empirical entropy estimator<br/>
is more than 10 times faster (processor: core i5 5765C with 128MB L3 cache).</p>

<p>The two entropy estimators have been tested on a variety of distributions as follows: 1000 samples of size 30000 and 3000 are<br/>
generated and the smaller sample padded to size 30000 as described in the original paper. The entropies of the samples are<br/>
then computed with both estimators and mean and variance of the estimate computed. These values are then compared with the<br/>
true entropy of the distribution generating the samples. In all cases tested the two estimators show similar performance,<br/>
see <a href="results/EntropyEstimation.csv">results</a>. </p>

</body>
</html>
